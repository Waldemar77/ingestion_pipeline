{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< Importing main libreries >>>\n",
    "\n",
    "Note:\n",
    "Before start, make sure you have your csv files into \"dataPruebaDataEngineer\" directory.\n",
    "The main idea about this ingestion pipeline is identify the .csv files into a specific directory, then, clean data and save it into a PostgreSQL database (localhost in this case), showing some stadistics outcomes like total rows saved into a database, average, minimum value and maximum value of price field for each file charged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< Creating read schemas for .csv files >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< Starting SparkSession and configurating PostgreSQL Connection >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.getOrCreate().stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"[PragmaTest] CSV Ingestion\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/pragma_assessment\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"pragma1234\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "#print(spark._jvm.System.getProperty(\"java.class.path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< Creating function with ingestion logic >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source directory where new csv files are coming\n",
    "src_path = 'dataPruebaDataEngineer'\n",
    "\n",
    "# destination path to move the processed files from source directory\n",
    "# the main idea is processing each csv file and move it to keep clean source directorory\n",
    "dest_path = 'processed_files'\n",
    "\n",
    "# Ensuring that destination directoru exists or create if it isn't exist.\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# function to clean data\n",
    "def cleaning_data(df_to_clean):\n",
    "    # 1. cleaning data: if price value is null, we set it \"0\"\n",
    "    # 2. delete header row\n",
    "    df_cleaned = df_to_clean.fillna({\"price\":0})\n",
    "    row_header = df_cleaned.first()\n",
    "    df_cleaned = df_cleaned.filter(df_cleaned[0] != row_header[0])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# function to process csv files\n",
    "def processing_file(file_path):\n",
    "    # reading csv file and creating a dataframe\n",
    "    csv_df = spark.read.schema(read_csv_schema).csv(file_path)\n",
    "\n",
    "    # cleaning data through function \"cleaing_data()\"\n",
    "    csv_cleaned = cleaning_data(csv_df)\n",
    "\n",
    "    # saving df into postgresql database\n",
    "    csv_cleaned.write.jdbc(url=postgres_url, table=\"pragma_prices\", \n",
    "                      mode=\"append\", properties=properties)\n",
    "    \n",
    "    return csv_cleaned\n",
    "\n",
    "# function to show total rows or records\n",
    "def total_rows(df_to_process):\n",
    "    # >>> total count about rows of dataframe\n",
    "    total_rows_count = df_to_process.count()\n",
    "\n",
    "    return total_rows_count\n",
    "\n",
    "# function to calculate price avarage\n",
    "def price_avg(df_to_process):\n",
    "    # price avg\n",
    "    df_to_process = df_to_process.toPandas()\n",
    "    price_avg = df_to_process[\"price\"].astype(int).mean()\n",
    "    \n",
    "    return price_avg\n",
    "\n",
    "# function to show the minimum value of price field\n",
    "def price_min(df_to_process):\n",
    "    # price minimum\n",
    "    df_to_process = df_to_process.toPandas()\n",
    "    price_min = df_to_process[\"price\"].astype(int).min()\n",
    "    \n",
    "    return price_min\n",
    "\n",
    "# function to show the maximum value of price field and moving file\n",
    "def price_max(df_to_process):\n",
    "    # price maximum\n",
    "    df_to_process = df_to_process.toPandas()\n",
    "    price_max = df_to_process[\"price\"].astype(int).max()\n",
    "\n",
    "    return price_max\n",
    "\n",
    "def moving_file_processed(file_path):\n",
    "    # moving processed csv file to processed directory\n",
    "    shutil.move(file_path, os.path.join(dest_path, os.path.basename(file_path)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< Executing ingestion process >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Total rows processed and saved: 22\n",
      ">>> Average of 'Price' field: 54.22727272727273\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 97\n",
      ">>> Total rows processed and saved: 51\n",
      ">>> Average of 'Price' field: 54.527429467084644\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 100\n",
      ">>> Total rows processed and saved: 82\n",
      ">>> Average of 'Price' field: 56.24409276300267\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 100\n",
      ">>> Total rows processed and saved: 112\n",
      ">>> Average of 'Price' field: 55.57473623891867\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 100\n",
      ">>> Total rows processed and saved: 143\n",
      ">>> Average of 'Price' field: 56.111401894360746\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 100\n",
      ">>> Total rows processed and saved: 151\n",
      ">>> Average of 'Price' field: 53.71783491196729\n",
      ">>> Minumum value of 'Price' field: 0\n",
      ">>> Maximum value of 'Price' field: 100\n"
     ]
    }
   ],
   "source": [
    "# list names of source path and process each file (one by one)\n",
    "name_files = os.listdir(src_path)\n",
    "\n",
    "# for each file name found in name files list, we validate if \n",
    "# that one is csv file.\n",
    "# Then, creating file path and executing function to process each file found.\n",
    "file_processed = 2\n",
    "total_count = 0\n",
    "all_avg = []\n",
    "all_min = []\n",
    "all_max = []\n",
    "\n",
    "for file in name_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(src_path, file)\n",
    "        df_saved = processing_file(file_path)\n",
    "        \n",
    "        # Showing Stadistics \n",
    "        # >>> count\n",
    "        total_count = total_count + total_rows(df_saved)\n",
    "        print(f\">>> Total rows processed and saved: {total_count}\")\n",
    "\n",
    "        # price avg\n",
    "        all_avg.append(price_avg(df_saved))\n",
    "        new_avg = sum(all_avg) / len(all_avg)\n",
    "        print(f\">>> Average of 'Price' field: {new_avg}\")\n",
    "\n",
    "        # min value of 'price'\n",
    "        all_min.append(price_min(df_saved))\n",
    "        min_value = min(all_min)\n",
    "        print(f\">>> Minumum value of 'Price' field: {min_value}\")\n",
    "        \n",
    "        # max value of 'price'\n",
    "        all_max.append(price_max(df_saved))\n",
    "        max_value = max(all_max)\n",
    "        print(f\">>> Maximum value of 'Price' field: {max_value}\")\n",
    "\n",
    "        # moving file to processed_files directory\n",
    "        moving_file_processed(file_path)\n",
    "        \n",
    "        file_processed += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
